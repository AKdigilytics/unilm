{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitextractionvirtualenv7ef0136161cc4e379db2b1935908e6c8",
   "display_name": "Python 3.7.3 64-bit ('extraction': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    batch = [i for i in zip(*data)]\n",
    "    for i in range(len(batch)):\n",
    "        if i < len(batch) - 2:\n",
    "            batch[i] = torch.stack(batch[i], 0)\n",
    "    return tuple(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "    if \"O\" not in labels:\n",
    "        labels = [\"O\"] + labels\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "\n",
    "### Train Sampler\n",
    "\n",
    "- [ ] What is a Sampler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = (\n",
    "        RandomSampler(train_dataset)\n",
    "        if args.local_rank == -1\n",
    "        else DistributedSampler(train_dataset)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataloader\n",
    "\n",
    "- [ ] What is a Dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=args.train_batch_size,\n",
    "        collate_fn=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = (\n",
    "            args.max_steps\n",
    "            // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            + 1\n",
    "        )\n",
    "    else:\n",
    "        t_total = (\n",
    "            len(train_dataloader)\n",
    "            // args.gradient_accumulation_steps\n",
    "            * args.num_train_epochs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] What is the deal with these double slashes?\n",
    "\n",
    "Some kind of division for [integer output](https://stackoverflow.com/questions/1535596/what-is-the-reason-for-having-in-python#1535601)\n",
    "\n",
    "Still need to figure out the meanings of `dataloader`, `gradient_accumulation_steps`.\n",
    "\n",
    "- [ ] Is there any relation between `args.max_steps` and `args.num_train_ephochs`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(\n",
    "            model, optimizer, opt_level=args.fp16_opt_level\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Half Precision arithmetic\n",
    "\n",
    "Mostly for efficiency purposes.  \n",
    "Check out [this](https://en.wikipedia.org/wiki/Half-precision_floating-point_format), [this](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) and [this](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) to know about what floating point precision means.  \n",
    "\n",
    "However for our specific problem, we're considered with what half precision means for training. This is elaborated [here](https://developer.nvidia.com/blog/apex-pytorch-easy-mixed-precision-training/).   \n",
    "Half precision is also where NVIDIA's apex library becomes a dependency.\n",
    "\n",
    "However this looks optional. Seems like one could skip this and just work with the defaults. However that might also remove support for some other optimizations needed later, such as parallel and distributed training, as is mentioned in the next steps in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model,\n",
    "            device_ids=[args.local_rank],\n",
    "            output_device=args.local_rank,\n",
    "            find_unused_parameters=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper logs that can be ignored for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\n",
    "        \"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size\n",
    "    )\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now we were setting up the infrastructure or hardware. We were making PyTorch aware of our operational or infrastructure preferences: how many GPUs, half or single point arithmetic, etc. After this point, we're going past the operational setup to be used, and making the architectural level decisions for setting up and training the neural network. So you'll start to see some terms which are related to Neural Network and Deep Learning concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each of these things one by one. \n",
    "\n",
    "1. Read up about `model.zero_grad`.   \n",
    "[This](https://stackoverflow.com/a/48009142/3727642) is a useful discussion because it also talks about `loss.backwards` which we'll encounter later.\n",
    "\n",
    "2. `trange()` seems to be just a helper/convenient function. Nothing to understand here, it seems like a wrapper around/prozy for [**tqdm**](https://tqdm.github.io/).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There seem to be two loops to be run here:\n",
    "1. Training Loop\n",
    "2. Epoch Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(\n",
    "            train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, the input dictionary is set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0].to(args.device),\n",
    "                \"attention_mask\": batch[1].to(args.device),\n",
    "                \"labels\": batch[3].to(args.device),\n",
    "            }\n",
    "            if args.model_type in [\"layoutlm\"]:\n",
    "                inputs[\"bbox\"] = batch[4].to(args.device)\n",
    "            inputs[\"token_type_ids\"] = (\n",
    "                batch[2].to(args.device) if args.model_type in [\"bert\", \"layoutlm\"] else None\n",
    "            )  # RoBERTa don\"t use segment_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Describe the input dictionary and the significance of each of its key-value pairs \n",
    "\n",
    "The crux of the loop is in a single line, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Similarly, describe the Output dictionary and the significance of each of its key-value pairs.\n",
    "\n",
    "In the following code, collect the output from the model run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth looking into the following individually: \n",
    "- [ ] `loss.backward()`: \n",
    "- [ ] `scaled_loss.backward()`:\n",
    "- [ ] `args.gradient_accumulation_steps`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}